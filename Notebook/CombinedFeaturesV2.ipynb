{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "alien-stranger",
   "metadata": {
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1618167125588,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "brown-bronze"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import gensim\n",
    "import random\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-massachusetts",
   "metadata": {
    "id": "graphic-addiction"
   },
   "source": [
    "### Vader Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cordless-jacket",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "error",
     "timestamp": 1618167232436,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "stylish-vietnamese",
    "outputId": "a2fab05e-f887-4db1-a4cf-ebc7ec26ed35"
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv('Clean_v1.csv')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def vaderScore(comments):\n",
    "    comments = re.sub('\\w+:\\/\\/\\S+','',comments)\n",
    "    comments = re.sub('\\.\\.\\.','', comments)\n",
    "    comments = re.sub('[0-9]+','', comments)\n",
    "    comments = re.sub('\\|\\|\\|', '', comments)\n",
    "    comments = sia.polarity_scores(comments)\n",
    "    return comments\n",
    "\n",
    "df1['vaderScore']=df1['posts'].apply(lambda x: vaderScore(x))\n",
    "vaderDF =df1[[\"Unnamed: 0\",'type','vaderScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-pledge",
   "metadata": {
    "id": "basic-hamilton"
   },
   "source": [
    "### Emoji Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bizarre-trauma",
   "metadata": {
    "id": "about-amber"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "tired-pencil",
   "metadata": {
    "id": "concrete-pointer"
   },
   "outputs": [],
   "source": [
    "words = df['New']\n",
    "posts = []\n",
    "for i in range(len(words)):\n",
    "    pre = words[i]\n",
    "    lst = eval(pre)\n",
    "    post = ''\n",
    "    for word in lst:\n",
    "        post += word + ' '\n",
    "    posts.append(post)\n",
    "\n",
    "emotes_list = []\n",
    "for i in posts:\n",
    "    emotes = re.findall('\\<.{2,25}?\\>', i)\n",
    "    for i in emotes:\n",
    "        if i.count('<')+i.count('>')>2: #more filters\n",
    "            emotes.remove(i)\n",
    "    for j in emotes:\n",
    "        emotes_list.append(j)\n",
    "fd = nltk.FreqDist(emotes_list)\n",
    "common_emojis = [x for x,y in fd.most_common(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "advised-explorer",
   "metadata": {
    "id": "split-cameroon"
   },
   "outputs": [],
   "source": [
    "def emojiClean(comments):\n",
    "    comments = re.sub(\"\\[\", '',comments)\n",
    "    comments = re.sub(\"\\]\", '',comments)\n",
    "    comments = re.sub(\"\\'\", '',comments)\n",
    "    comments = re.sub(\"\\,\", '',comments)\n",
    "    return comments\n",
    "\n",
    "def feature_extractor(doc):\n",
    "    features = {}\n",
    "    for e in common_emojis:\n",
    "        features[e] = (e in doc)\n",
    "    return features\n",
    "\n",
    "df['Emoji']=df['New'].apply(lambda x: emojiClean(x))\n",
    "df['EmojiFeature'] = df['Emoji'].apply(lambda x: feature_extractor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-adoption",
   "metadata": {
    "id": "correct-letters"
   },
   "source": [
    "### Common ADJ Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "expired-grass",
   "metadata": {
    "id": "dangerous-planning"
   },
   "outputs": [],
   "source": [
    "stopwordlist = gensim.parsing.preprocessing.STOPWORDS\n",
    "def cleaner(post):\n",
    "    # remove words in < > or special symbols residuals\n",
    "    special_symbols_remove = re.sub('\\<[A-Za-z]+.[\\s\\w]*\\>|\\(|\\)|\\*|[A-Z]\\/[A-Z]|\\_+', '', post)\n",
    "    wordlist = nltk.word_tokenize(special_symbols_remove)\n",
    "    words_nonstop = [word.lower() for word in wordlist if word.lower() not in stopwordlist or word not in string.punctuation]\n",
    "    words = [word for word in words_nonstop if len(word) > 3] # words that length than 3\n",
    "    return words\n",
    "\n",
    "def feature_common_adjs(words):\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    adjs = [x for x, y in tagged if y.startswith('JJ') and x.isalpha()] # prevent uncleaned words\n",
    "    common_adjs = [x for x, y in nltk.FreqDist(adjs).most_common(200)]\n",
    "    return common_adjs\n",
    "\n",
    "def check_adjs(post):\n",
    "    adj_in_type = [w for w in post if w in adjs]\n",
    "    return {'AdjInTop200 ': len(adj_in_type)}\n",
    "\n",
    "df['Cleaner'] = df['Clean'].apply(lambda post: cleaner(post))\n",
    "corpus = [word for post in df['Cleaner'] for word in post] # total 5994734\n",
    "adjs = feature_common_adjs(corpus)\n",
    "df['ADJs'] = df['Cleaner'].apply(lambda post: check_adjs(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-blackberry",
   "metadata": {
    "id": "numerical-telescope"
   },
   "source": [
    "### Most Common Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "hired-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word = []\n",
    "for i in range(len(df['Clean'])):\n",
    "    word = cleaner(df['Clean'][i])\n",
    "    for j in word:\n",
    "        all_word.append(j)\n",
    "       \n",
    "fd = nltk.FreqDist(all_word)      \n",
    "most_common = fd.most_common(200)\n",
    "most = [z[0] for z in most_common]\n",
    "\n",
    "# Set the feature\n",
    "def most_feature(post):\n",
    "    count = 0\n",
    "    words = set(post)\n",
    "    feature = {}\n",
    "    for w in words:\n",
    "        if w in most:\n",
    "            count = count + 1\n",
    "            feature['CommonWordCount'] = count\n",
    "        else:\n",
    "            feature['CommonWordCount'] = count\n",
    "    return feature\n",
    "\n",
    "df['MostCommonWord'] = df['Cleaner'].apply(lambda post: most_feature(post))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-prisoner",
   "metadata": {},
   "source": [
    "### Avg Length in the Sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "suitable-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sent):\n",
    "    str1 = sent.strip() # remove the space at start and end of string\n",
    "    index = 0\n",
    "    count = 0\n",
    "    \n",
    "    while index < len(str1):\n",
    "        while str1[index] != \" \": \n",
    "            index += 1   \n",
    "            if index == len(str1): \n",
    "                break\n",
    "        count += 1 # count number of word\n",
    "        if index == len(str1): # check if the character is last one\n",
    "            break\n",
    "        while str1[index] == \" \":  # check the space between the word\n",
    "            index += 1\n",
    "    \n",
    "    # the number of sentence of post\n",
    "    num_sent = len(re.split(r'[.!?]+',sent))\n",
    "    # calculate the average word per sentence\n",
    "    avg_word_length = count/num_sent\n",
    "\n",
    "    return round(avg_word_length,1)\n",
    "\n",
    "def avg_feature(num):\n",
    "    feature = {} \n",
    "    feature['AvgSentLength'] = num\n",
    "    return feature\n",
    "\n",
    "df['AvgSentLength'] = df['Clean'].apply(lambda post: avg_word(post))\n",
    "df['AvgSentLength'] = df['AvgSentLength'].apply(lambda post: avg_feature(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-navigator",
   "metadata": {},
   "source": [
    "### TFIDF Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-knock",
   "metadata": {
    "id": "written-cuisine",
    "outputId": "aba12b51-8bb5-466b-9b4e-4f36eecca732"
   },
   "outputs": [],
   "source": [
    "### Run if you have 18 hrs to spare.... if not just import from pickle file: PickleFeatures\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf=vectorizer.fit_transform(posts)\n",
    "num_word=len(vectorizer.get_feature_names())\n",
    "\n",
    "def feature_function(post_index):\n",
    "    sum_tfidf=sum([tfidf[post_index,word_index] for word_index in range(0,num_word)])\n",
    "    length=len(nltk.word_tokenize(posts[post_index]))\n",
    "    return {'avg_tfidf': round(sum_tfidf,3)}\n",
    "\n",
    "lst =[]\n",
    "for i in range(8675):\n",
    "    lst.append(feature_function(i))\n",
    "    \n",
    "df[\"Avg TFIDF\"]=lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-letters",
   "metadata": {},
   "source": [
    "### AVG Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "southern-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgChar(doc):\n",
    "    lst = eval(doc)\n",
    "    lst = [x for x in lst if x.isalpha() and len(x)>2]\n",
    "    word_no = len(lst)\n",
    "    count = 0\n",
    "    for i in lst:\n",
    "        count += len(i)\n",
    "    try:\n",
    "        avgChar = count/word_no\n",
    "    except ZeroDivisionError:\n",
    "        avgChar = 0\n",
    "    return {'AvgChar': round(avgChar,2)}\n",
    "df['AvgChar'] = df['New'].apply(lambda x: avgChar(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-dubai",
   "metadata": {},
   "source": [
    "### Joining the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "curious-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the df and vaderDF because they are both using different clean files.\n",
    "dfMerge = pd.merge(df, vaderDF, on=['Unnamed: 0','type'], how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "joined-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this instead of running the AVG TFIDF FUNCTION\n",
    "pickleDF = pd.read_pickle('PickleFeatures')\n",
    "pickleDF =pickleDF[['Unnamed: 0','type','Avg TFIDF']]\n",
    "dfMerge = pd.merge(dfMerge, pickleDF, on=['Unnamed: 0','type'], how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "athletic-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subeset dfMerge and grabbed only the features ouput\n",
    "featuresDF=dfMerge[['type','EmojiFeature','ADJs','AvgSentLength','MostCommonWord','AvgChar', 'Avg TFIDF','vaderScore']]\n",
    "featuresDF.to_pickle(\"PickleFeaturesV2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-ethics",
   "metadata": {},
   "source": [
    "### Combing Features to Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('PickleFeaturesV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a new column to df\n",
    "df['CombineDict']= ''\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df['CombineDict'][i]= ({**df['EmojiFeature'][i], **df['ADJs'][i],**df['AvgSentLength'][i],**df['MostCommonWord'][i],\n",
    "                            **df['AvgChar'][i], **df[\"Avg TFIDF\"][i], **df['vaderScore'][i]}, df['type'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"PickleFeatureSet\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CombinedFeatures.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
